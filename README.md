# HumanEval+ Iterative Coding Environment

A simple, streamlined environment for testing LLM agents on HumanEval+ coding problems with iterative feedback.

## ğŸ¯ **Simple Workflow**

1. **Setup Task**: Load a HumanEval+ task and get its details
2. **Generate Code**: LLM generates code (no file I/O needed)
3. **Extract Code**: Post-process the response to extract clean code
4. **Check Correctness**: Test the code against test cases

## ğŸš€ **Quick Start**

### 1. Test Tools
```bash
cd /Users/martinweyssow/Documents/hud-python/hud-humanevalplus
uv run test_simple_tools.py
```

### 2. Run Simple Agent
```bash
uv run simple_client.py
```

### 3. Run Multiple Tasks
```bash
uv run simple_client.py multiple
```

## ğŸ› ï¸ **Available Tools**

### `list_tasks()`
- Lists available HumanEval+ tasks
- Returns task IDs and descriptions

### `setup_task(task_id: str)`
- Loads a specific HumanEval+ task
- Returns task details (prompt, test cases, etc.)
- Sets the current task for testing

### `check_correctness(code: str)`
- Tests generated code against test cases
- Uses temporary directory for isolation
- Returns detailed test results and success rate

## ğŸ“Š **Example Output**

```
ğŸš€ Solving task: HumanEval/0
==================================================
ğŸ”Œ Initializing MCP client...
âœ… Client initialized successfully
ğŸ“‹ Step 1: Setting up task HumanEval/0...
âœ… Task loaded: Task HumanEval/0 loaded successfully
ğŸ“ Prompt: Complete the function has_close_elements...
ğŸ¤– Step 2: Generating code with LLM...
âœ… Code generated (245 chars)
ğŸ” Step 3: Extracting code from response...
âœ… Code extracted (156 chars)
ğŸ§ª Step 4: Checking correctness...
ğŸ“Š Step 5: Results
âœ… Success: True
ğŸ“ˆ Success Rate: 100.0%
âœ… Passed: 8
âŒ Failed: 0
ğŸ“Š Total: 8
ğŸ’¬ Message: Tests passed: 8/8 (100.0%)
```

## ğŸ¯ **Key Benefits**

- **Simple**: No complex file I/O or workspace management
- **Isolated**: Each test runs in a temporary directory
- **Fast**: Direct code testing without file operations
- **Clean**: Clear separation between generation and testing
- **Extensible**: Easy to add iterative refinement later

## ğŸ”„ **Future Enhancements**

- Add iterative refinement where agent can see test failures
- Add more sophisticated code extraction
- Add support for multiple programming languages
- Add performance metrics and benchmarking

## ğŸ—ï¸ **Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MCP Client    â”‚    â”‚   MCP Server     â”‚    â”‚  Context Server â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â€¢ setup_task    â”‚â—„â”€â”€â”€â”¤ â€¢ setup_task     â”‚â—„â”€â”€â”€â”¤ â€¢ HumanEval+    â”‚
â”‚ â€¢ check_correct â”‚â—„â”€â”€â”€â”¤ â€¢ check_correct  â”‚    â”‚   Dataset       â”‚
â”‚ â€¢ list_tasks    â”‚â—„â”€â”€â”€â”¤ â€¢ list_tasks     â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

This demonstrates the core value of **environment evaluation** - agents can test their solutions and get feedback, enabling iterative improvement! ğŸš€